---
layout: page
permalink: /papers/
title: Interesting Papers
---
This is a collection of papers I've read and enjoyed and feel like have the most bang for buck in understanding the current DL / LLM landscape. 

<br>

### Foundational Concepts and Architectures

- [[1986] Learning representations by back-propagating errors](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)
- [[1989] Handwritten Digital Recognition with a Back-Propagation Network](https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf)
- [[2012] ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [[2013] Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- [[2014] Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [[2014] Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
- [[2017] Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [[2018] Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/abs/1801.10198)
- [[2018] Improving Language Understand by Generative Pre-Training](https://openai.com/index/language-unsupervised/)

<br>

### Network Stability / Regularization Techniques

- [[2014] Dropout: A Simple Way to Prevent Neural Networks from
Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
- [[2015] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

<br>

### Fine-Tuning & PEFT

- [[2012] LoRa: Low-Rank Adaption of Large Language Models](https://arxiv.org/abs/2106.09685)
- [[2023] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)

<br>

### **Production and Deployment**

- [[2023] Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)