---
layout: post
title: A brief Introduction to LLMOps
categories: [AI]
---


* Do not remove this line (it will not be displayed)
{:toc}

<br>

# What is LLMOps ? A Rebranded MLOps or Something Fundamentally Different ?

With the rise of large language models and as business rush to try capitalise on this new opportunity, but traditional ML practices have begun to run into new challenges not yet seen, or at least far less common until now.

<br>

Youâ€™re probably asking yourself, isnâ€™t LLMOps the same things as MLOps ? Why do we need a new term or set of principles for something thatâ€™s largely so similar. Well, Iâ€™d like to argue, that while yes they have a lot of overlap, they are indeed different enough to make it with differentiating between them, and it is a new (but familiar) skill-set that operators must learn to deal with. 

<br>

So letâ€™s look at the key components of LMOps, explore how it differs from traditional MLOps (and where it overlaps), and some of the unique challenges faced in this emerging field.

<br>

# Key Components & Differences from MLOps 

If we take a quick look the flow of an LLMOps model you will notice a lot of similarities from regular MLOps processes but also some stark differences. 

<img src="/images/llm-ops/llm-ops-flow.png" alt="LLMOps Flow"/>
<p align="center">
  <a href="https://valohai.com/blog/llmops/">Image Source</a>
</p>

<br>

## Training

### LLMs are Impractical to Train From Scratch

Youâ€™ll see straight away, that while there is still training, in LLMOps foundational models are so big and complex to train that the ability to do so lies far beyond the reach of most businesses. Therefore as practitioners we must become skilled in finding and choosing the right foundational model for our needs and adopting it. Here the skill of fine tuning with the right dataset and right fine tuning technique become important. 

<br>

### LLMs require Adapting Foundation Models to Specific Use Cases

The general process today looks like pulling a shared model of a public repository, HuggingFace  realistically being the only provider in this space right now. From here you would have a clean dataset from your company that you would want to fine tune your model on (you can choose my modality, and tasks, languages etc..). Ideally you want to find the correct model for your use case, not only in itâ€™s pre-trained abilities but for itâ€™s size so you can it run cost effectively. 

From here you can fine tune your model with your data. Thereâ€™s a whole ecosystem of fine tuning techniques to choose from, each with their own pros and cos, but for now the easiest one to grab is something like LoRa. You can checkout out my article on [Parameter Efficient Fine Tuning](../model-fine-tuning/) to learn more on the topic.

<br>

## Serving

With LLMs, deployment becomes a different beast. Not only are these models are much more computational resource intensive, but they bring in a host of new problems. For instance how do we measure latency now ?

<br>

### LLMs Are Resource Intensive

To run these models effectively you need special software to optimise at serving time, specifically tackling things like:

- **Memory Optimization**: Transformers are memory-bound for inference, so these servers come with a whole range of specific optimizations to efficiently manage memory usage and reduce the memory footprint.
- **Batching**: Regular batching strategies donâ€™t adapt optimally to Transformers. Better strategies exist, such as continuous batching, which aggregates requests dynamically to maximize GPU utilization and minimize latency.
- **Parallelization**: Efficiently distributing the computation across multiple GPUs or CPU cores is crucial. Parallelization strategies include model parallelism, where the model is split across different devices, and data parallelism, where multiple devices process different batches of data simultaneously.
- **Specific GPU Support**: Leveraging the advanced features of modern GPUs, such as tensor cores, mixed precision training, and optimized libraries (e.g., CUDA, cuDNN), ensures that inference is both fast and efficient, taking full advantage of hardware acceleration capabilities.

<br>

### LLMs Require Dedicated Serving Tools

To enable optimisations and serve these models efficiently a whole host of inference servers have appeared to cater to the new requirements:

- Triton Inference Server â†’ An offering from NVIDIA  which supports a number of DL frameworks and provides excellent support for GPU models & workloads
- vLLM â†’ A standalone inference server, can be used as a backend in Triton, which provides a number of optimisations such as PagedAttention for Transformer models.
- LLama.cp â†’ Specifically designed for deploying LLaMA models. Aimed to be easy to set up, purely in C/C++, so no dependencies, and comes with many optimisations for LLMs.
- Text Generation Inference > An offering from HuggingFace that integrates with their model repository. It provides support for both CPU and GPU deployments.

<br>

### LLMs Require New Metrics

LLMOps are not serving a single response per request, but a series of responses, and with this it brings in a host of new problems. We now have a host of new metrics such as:

- Time To First Token (TTFT)
- Time Per Output Token (TPOT)
- Latency - The time to serve the full response
- Throughput - The number of tokens generated p/second

<br>

For a deep dive on the specific of serving LLMs, you can check my post [LLM Model Inference](../llm-inference-1).

<br>

## Model Evaluation 

ðŸš§ Under Construction ðŸš§

## Governance & Security

ðŸš§ Under Construction ðŸš§

# Conclusion & Wrap Up

ðŸš§ Under Construction ðŸš§