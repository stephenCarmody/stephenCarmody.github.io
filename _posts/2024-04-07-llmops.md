---
layout: post
title: A brief Introduction to LLMOps
categories: [AI]
---


* Do not remove this line (it will not be displayed)
{:toc}

<br>

# What is LLMOps ? A Rebranded MLOps or Something Fundamentally Different ?

With the rise of large language models and as business rush to try capitalise on this new opportunity, but traditional ML practices have begun to run into new challenges not yet seen, or at least far less common until now.

<br>

Youâ€™re probably asking yourself, isnâ€™t LLMOps the same things as MLOps ? Why do we need a new term or set of principles for something thatâ€™s largely so similar. Well, Iâ€™d like to argue, that while yes they have a lot of overlap, they are indeed different enough to make it with differentiating between them, and it is a new (but familiar) skill-set that operators must learn to deal with. 

<br>

So letâ€™s look at the key components of LMOps, explore how it differs from traditional MLOps (and where it overlaps), and some of the unique challenges faced in this emerging field.

<br>

# Key Components & Differences from MLOps 

If we take a quick look the flow of an LLMOps model you will notice a lot of similarities from regular MLOps processes but also some stark differences. 

<img src="/images/llm-ops/llm-ops-flow.png" alt="LLMOps Flow"/>
<p align="center">
  <a href="https://valohai.com/blog/llmops/">Image Source</a>
</p>


## Training

### Foundational Models over Training From Scratch

Youâ€™ll see straight away, that while there is still training, in LLMOps foundational models are so big and complex to train that the ability to do so lies far beyond the reach of most businesses. Therefore as practitioners we must become skilled in finding and choosing the right foundational model for our needs and adopting it. Here the skill of fine tuning with the right dataset and right fine tuning technique become important. 

### Fine Tuning Foundation Models to Adapt to Needs

The general process today looks like pulling a shared model of a public repository, HuggingFace  realistically being the only provider in this space right now. From here you would have a clean dataset from your company that you would want to fine tune your model on (you can choose my modality, and tasks, languages etc..). Ideally you want to find the correct model for your use case, not only in itâ€™s pre-trained abilities but for itâ€™s size so you can it run cost effectively. 

From here you can fine tune your model with your data. Thereâ€™s a whole ecosystem of fine tuning techniques to choose from, each with their own pros and cos, but for now the easiest one to grab is something like LoRa. You can checkout out my article on [Parameter Efficient Fine Tuning](../model-fine-tuning/) to learn more on the topic.

<br>

## Serving

With LLMs, deployment becomes a different beast. Not only are these models are much more computational resource intensive, but they bring in a host of new problems. For instance how do we measure latency now ?

<br>

ðŸš§ Under Construction ðŸš§